{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: Estimating the Parameters of the Bass Model*\n",
    "Subtitle: Using Solver to Find the Best Fit for P, Q, and N\n",
    "Content:\n",
    "• How can we determine the optimal values for the Bass model parameters?\n",
    "   - Use Solver to find P, Q, and N that minimize the Sum of Squared Errors (SSE)\n",
    "      1. Input trial values for P, Q, and N\n",
    "      2. Calculate predicted sales using the Bass equation\n",
    "      3. Compute SSE between actual and predicted sales\n",
    "   - Solver adjusts P, Q, and N to minimize SSE\n",
    "      - Constraints: 0 ≤ P ≤ 1, 0 ≤ Q ≤ 1\n",
    "   - Relative magnitudes of P and Q indicate importance of innovation vs. imitation\n",
    "\n",
    "\n",
    "### *Title: Estimating Bass Model Parameters in Excel*\n",
    "Content:\n",
    "1. In cells E2:G2, input initial guesses for P, Q, and N. Name these cells p, q, and Nbar.\n",
    "\n",
    "2. In cell D6, enter the Bass model formula: \n",
    "   =p*(Nbar-C5)+(q/Nbar)*C5*(Nbar-C5)\n",
    "   Copy this formula down to cells D7:D21.\n",
    "\n",
    "3. In cell E6, calculate the squared error between predicted and actual sales:\n",
    "   =(B6-D6)^2 \n",
    "   Copy this formula down to cells E7:E21.\n",
    "\n",
    "4. In cell E3, calculate the Sum of Squared Errors (SSE) using:\n",
    "   =SUM(E6:E21)\n",
    "\n",
    "5. Open Solver and set it up as follows:\n",
    "   - Set Objective: E3\n",
    "   - To: Min\n",
    "   - By Changing Variable Cells: $E$2:$G$2\n",
    "   - Subject to the Constraints:\n",
    "      - $E$2:$F$2 >= 0\n",
    "      - $E$2:$F$2 <= 1\n",
    "   - Select GRG Nonlinear as the Solving Method\n",
    "   - Click Solve to find the optimal values of P, Q, and N that minimize SSE.\n",
    "\n",
    "\n",
    "\n",
    "### *This Python code does the following:*\n",
    "\n",
    "1. Imports necessary libraries (pandas, sqlite3, numpy, scipy.optimize)\n",
    "2. Loads sales data from the Excel file \"ColorTV.xlsx\" into a pandas DataFrame\n",
    "3. Creates an SQLite database \"ColorTV.db\" and a table \"sales_data\" from the DataFrame\n",
    "4. Retrieves data from the SQLite table into a new DataFrame for further analysis\n",
    "5. Defines the Bass model equation and the cost function (Sum of Squared Errors) to minimize\n",
    "6. Sets initial parameter guesses and bounds for P, Q, and M\n",
    "7. Uses scipy.optimize.minimize to find the optimal parameter values that minimize SSE\n",
    "8. Prints out the optimal values of P, Q, and M\n",
    "9. Closes the SQLite database connection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load data from Excel into a pandas DataFrame\n",
    "data = pd.read_excel('data/ColorTV.xlsx', sheet_name='Sheet1', usecols='A:C', skiprows=4, nrows=16)\n",
    "\n",
    "# Connect to SQLite database and create a table from the DataFrame\n",
    "conn = sqlite3.connect('data/ColorTV.db')\n",
    "data.to_sql('sales_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Retrieve data from SQLite into a new DataFrame\n",
    "query = 'SELECT * FROM sales_data'\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# Define the Bass model equation\n",
    "def bass_model(t, p, q, m):\n",
    "    a = p + q\n",
    "    b = p / m\n",
    "    c = np.exp(-(a * t))\n",
    "    return m * (((a**2)/p) * c) / (1 + ((q/p) * c))**2\n",
    "\n",
    "# Define the cost function to minimize (Sum of Squared Errors)\n",
    "def cost_function(params):\n",
    "    p, q, m = params\n",
    "    y_pred = bass_model(df['t'], p, q, m)\n",
    "    y_true = df['n(t)']\n",
    "    sse = np.sum((y_true - y_pred)**2)\n",
    "    return sse\n",
    "\n",
    "# Set initial parameter guesses and bounds\n",
    "initial_params = [0.03, 0.38, 100]\n",
    "bounds = [(0, 1), (0, 1), (0, None)]\n",
    "\n",
    "# Use scipy.optimize.minimize to find optimal parameters\n",
    "result = minimize(cost_function, initial_params, bounds=bounds, method='L-BFGS-B')\n",
    "p, q, m = result.x\n",
    "\n",
    "# Print optimal parameter values\n",
    "print(f'Optimal P: {p:.3f}')  \n",
    "print(f'Optimal Q: {q:.3f}')\n",
    "print(f'Optimal M: {m:.3f}')\n",
    "\n",
    "# Close the SQLite connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### *Title: Forecasting New Product Sales with the Bass Model* ###\n",
    "Subtitle: Leveraging Adjacent Categories to Predict Adoption\n",
    "Content: \n",
    "- **Analogous products** from adjacent sectors can guide new product forecasts\n",
    "  - Find a similar, mature product (e.g., color TV for DIRECTV)\n",
    "  - Use the analog's P and Q values with estimated N for the new product\n",
    "- **Intentions data** helps estimate initial adoption but often overstates demand\n",
    "  - Adjust using factors like affordability and availability \n",
    "  - *Apply a research-based fractional correction (k) to the stated purchase intent percentage*\n",
    "- **Goal Seek** in Excel reverse engineers key Bass model parameters\n",
    "  - Set N_bar so the model matches your adjusted 1-year adoption estimate\n",
    " \n",
    "\n",
    "### *Title: The Power of Empirical Generalization* ###\n",
    "Subtitle: Discovering Patterns that Transcend Individual Circumstances\n",
    "Content:\n",
    "- **Empirical generalization** identifies consistent patterns across contexts\n",
    "  - Described by mathematical, graphical, or symbolic methods\n",
    "  - Enables insights and predictions based on a small set of core drivers\n",
    "- **The Bass model** is a prime example in new product forecasting\n",
    "  - P (coefficient of innovation) and Q (coefficient of imitation) often hold steady\n",
    "  - *Patterns of early adoption and word-of-mouth spread repeat across products*\n",
    "- **Managers** can harness this power for data-driven decisions\n",
    "  - Recognize the prevalence and potential of empirical regularities\n",
    "  - Build robust models on a backbone of shared dynamics\n",
    "\n",
    "\n",
    "\n",
    "### *Title: Using Excel to Forecast DIRECTV Adoption with the Bass Model* ###\n",
    "Content:\n",
    "1. Enter 0 in cells F7 and G7 for the initial number of DIRECTV subscribers.\n",
    "2. In cell F8, enter the formula =(1/12)*(p*(Nbar-G7)+q*G7*(Nbar-G7)/Nbar) and copy down to F55. This estimates new subscribers each month using Equation 1, adjusting P and Q to monthly values.  \n",
    "3. In cell G8, enter the formula =G7+F8 and copy down to G55 to calculate cumulative subscribers by adding the previous total to the current month's new subscribers.\n",
    "4. Use Excel's Goal Seek (Data > What-If Analysis > Goal Seek) to find the Nbar value that makes the 1-year subscriber estimate (G19) match an exogenous projection (e.g., 1.32 million). \n",
    "  - Set cell: $G$19 \n",
    "  - To value: 1.32\n",
    "  - By changing cell: $F$4\n",
    "5. The value of Nbar that matches the 1-year estimate (e.g., 21.55 million for 1.32 million subscribers) can now be used in the Bass model to forecast subscribers at later timepoints (e.g., G55 for a 4-year estimate).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This Python script:\n",
    "1. Loads analogous product parameters (p, q) and an exogenous 1-year subscriber estimate \n",
    "2. Reads the Excel data into a pandas DataFrame\n",
    "3. Creates an SQLite3 database and populates a 'subscribers' table with the Excel data\n",
    "4. Defines functions to:\n",
    "   - Calculate Bass model estimates for given parameters\n",
    "   - Calculate the sum of squared errors (SSE) between estimates and actual data\n",
    "5. Uses iterative optimization to find the Nbar that minimizes SSE, matching the 1-year estimate\n",
    "6. Queries the database for the 4-year subscriber forecast based on the optimized Nbar\n",
    "7. Prints the optimal Nbar and 4-year forecast  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load analogous product parameters \n",
    "p = 0.059 \n",
    "q = 0.1463\n",
    "\n",
    "# Load exogenous 1-year estimate of new product subscribers\n",
    "subs_1yr = 1.32 \n",
    "\n",
    "# Load Excel data into a DataFrame \n",
    "df = pd.read_excel('data/DIRECTV.xlsx', sheet_name='Sheet1', usecols='A:G', nrows=56)\n",
    "\n",
    "# Initialize a SQLite3 database and connection\n",
    "conn = sqlite3.connect('data/DIRECTV.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a subscribers table and populate it with Excel data\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS subscribers\n",
    "               (t INTEGER, actual REAL, cumulative REAL)''')\n",
    "          \n",
    "for index, row in df.iterrows():\n",
    "    t = index - 6\n",
    "    actual = row['F'] \n",
    "    cumulative = row['G']\n",
    "    cur.execute(\"INSERT INTO subscribers VALUES (?,?,?)\", (t, actual, cumulative))\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Function to calculate Bass model estimate for a given t and Nbar\n",
    "def bass_model(t, Nbar, actual, cumulative):\n",
    "    return (1/12) * (p * (Nbar - cumulative) + q * actual * (Nbar - cumulative) / Nbar)\n",
    "\n",
    "# Function to calculate SSE between Bass model estimates and DIRECTV data\n",
    "def calc_sse(Nbar):\n",
    "    cur.execute(\"SELECT * FROM subscribers\")\n",
    "    data = cur.fetchall()\n",
    "    \n",
    "    sse = 0\n",
    "    for row in data:\n",
    "        t = row[0]\n",
    "        actual = row[1]\n",
    "        cumulative = row[2]\n",
    "        \n",
    "        estimate = bass_model(t, Nbar, actual, cumulative)\n",
    "        sse += (estimate - actual)**2\n",
    "        \n",
    "    return sse\n",
    "\n",
    "# Optimize Nbar to match 1-year subscriber estimate    \n",
    "Nbar = 1\n",
    "step = 10000000\n",
    "\n",
    "while True:\n",
    "    sse_curr = calc_sse(Nbar)\n",
    "    sse_next = calc_sse(Nbar + step)\n",
    "    \n",
    "    if sse_next > sse_curr:\n",
    "        step /= 2\n",
    "    else:\n",
    "        Nbar += step\n",
    "        \n",
    "    if step < 0.001:\n",
    "        break\n",
    "        \n",
    "print(f\"Optimal Nbar: {Nbar:.2f}\")\n",
    "\n",
    "cur.execute(\"SELECT cumulative FROM subscribers WHERE t = 48\")\n",
    "subs_4yr = cur.fetchone()[0]\n",
    "print(f\"4-year subscriber forecast: {subs_4yr:.2f} million\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: How Uncertain are New Product Sales?* ###\n",
    "Subtitle: Combining the Bass Model with Monte Carlo Simulation  \n",
    "Content:\n",
    "- **New product sales** are highly uncertain\n",
    "  - Difficult to predict market size and adoption patterns\n",
    "  - Traditional point forecasts may give false sense of precision\n",
    "- **Monte Carlo simulation** models uncertain quantities  \n",
    "  - Represents inputs as probability distributions rather than single values\n",
    "  - *Generates range of possible outcomes to capture inherent uncertainty*\n",
    "- **Bass model parameters** can be treated as random variables\n",
    "  - Market potential (m): often modeled as a normal distribution\n",
    "  - *Coefficients of innovation (p) and imitation (q): drawn from analogous products*\n",
    " \n",
    "\n",
    "### *Title: Illuminating the Darkness of Uncertainty* ###\n",
    "Subtitle: Why Simulation Beats Point Forecasts for New Products\n",
    "Content:\n",
    "- **Point forecasts** give illusion of certainty and precision\n",
    "  - Single \"best guess\" values for market size, adoption rates, etc.\n",
    "  - But reality is often far more uncertain, especially for new products\n",
    "- **Simulation** embraces and quantifies inherent uncertainty\n",
    "  - Presents a realistic range of possibilities \n",
    "  - *Helps identify key drivers of variation in outcomes*\n",
    "  - Facilitates robust decision making under uncertainty\n",
    "- **Combining simulation with the Bass model** is especially powerful\n",
    "  - Acknowledges our imperfect knowledge of key parameters\n",
    "  - *Leverages data from analogous products to constrain estimates*\n",
    "  - Yields actionable insights for managing new product launches\n",
    "\n",
    "\n",
    "### *Title: Simulating New Product Sales with the Bass Model in Excel* ###\n",
    "Content:\n",
    "1. In cell C4, simulate potential market size (simNbar) with the formula =ROUND(NORMINV(RAND(),C2,C3),0), assuming a normal distribution with mean in C2 and standard deviation in C3.\n",
    "2. In cell H1, choose a random Bass parameter scenario (1-12) with =RANDBETWEEN(1,12).\n",
    "3. In cell H2, look up the P value for the chosen scenario with =VLOOKUP(scenario, B6:D17,2).\n",
    "4. In cell H3, look up the Q value with =VLOOKUP(scenario, B6:D17,3).\n",
    "5. In cell I6, enter 0 for initial cumulative sales. \n",
    "6. In I7, calculate cumulative sales with =SUM($H$6:H7). Copy this formula down to I18.\n",
    "7. In H7, calculate year 1 sales with =simp*(simNbar-I6)+simq*I6*(simNbar-I6)/simNbar. Copy this formula down to H18.\n",
    "8. Set up a data table to simulate 1,000 iterations:\n",
    "   - In cells F22, G22, and H22, reference 1-year (I7), 5-year (I11), and 10-year (I16) cumulative sales.\n",
    "   - Select E22:H1022 and go to Data > What-If Analysis > Data Table.\n",
    "   - Leave the Row Input cell blank and select any blank cell for the Column Input. \n",
    "9. Calculate average sales in F20 with =AVERAGE(F23:F1022). Copy this to G20:H20. \n",
    "10. Calculate standard deviations in F21 with =STDEV(F23:F1022). Copy this to G21:H21.\n",
    "\n",
    "The data table simulates 1,000 possible sales trajectories, capturing uncertainty in market potential and Bass parameters. The averages and standard deviations of 1-, 5-, and 10-year sales provide a realistic range of outcomes to inform decision making.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### *This Python script:* ###\n",
    "1. Reads Bass model parameters and scenarios from the Excel file into a pandas DataFrame.\n",
    "2. Creates an SQLite database and populates a 'scenarios' table with the Bass parameters.\n",
    "3. Defines functions to:\n",
    "   - Simulate market potential from a normal distribution.\n",
    "   - Retrieve Bass parameters for a randomly selected scenario from the database.\n",
    "   - Simulate the Bass model for given parameters and market potential.\n",
    "   - Run n iterations of the Bass model simulation.\n",
    "4. Simulates 1000 iterations of the Bass model for 10 periods.\n",
    "5. Prints the average and standard deviation of cumulative sales for periods 1, 5, and 10.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Read Bass parameters and scenarios from Excel\n",
    "df = pd.read_excel('data/Basssim.xlsx', sheet_name='Sheet1', usecols='B:D', skiprows=5, nrows=12) \n",
    "\n",
    "# Initialize SQLite database and connection\n",
    "conn = sqlite3.connect('data/Basssim.db')\n",
    "\n",
    "# Create scenarios table and insert Bass parameters\n",
    "df.to_sql('scenarios', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Simulate market potential from normal distribution\n",
    "def simulate_market_potential(mean, sigma):\n",
    "    return round(np.random.normal(mean, sigma))\n",
    "\n",
    "# Retrieve Bass parameters for a random scenario\n",
    "def get_bass_params(conn):\n",
    "    query = '''\n",
    "        SELECT p, q \n",
    "        FROM scenarios\n",
    "        WHERE \"Product\" = (SELECT \"Product\" \n",
    "                           FROM scenarios\n",
    "                           ORDER BY RANDOM()\n",
    "                           LIMIT 1)\n",
    "    '''\n",
    "    params = pd.read_sql_query(query, conn).values[0]\n",
    "    return params[0], params[1]\n",
    "\n",
    "# Simulate Bass model for given parameters and market potential\n",
    "def bass_model(p, q, market_potential, num_periods):\n",
    "    sales = np.zeros(num_periods)\n",
    "    cum_sales = np.zeros(num_periods)\n",
    "    \n",
    "    for i in range(num_periods):\n",
    "        if i == 0:\n",
    "            sales[i] = p * market_potential\n",
    "        else:\n",
    "            sales[i] = (p + q * cum_sales[i-1] / market_potential) * (market_potential - cum_sales[i-1])\n",
    "        cum_sales[i] = cum_sales[i-1] + sales[i]\n",
    "    \n",
    "    return sales, cum_sales\n",
    "\n",
    "# Simulate n iterations of the Bass model\n",
    "def simulate_bass(n, mean, sigma, num_periods):\n",
    "    results = pd.DataFrame(columns=[f'Period {i+1}' for i in range(num_periods)])\n",
    "    \n",
    "    for _ in range(n):\n",
    "        market_potential = simulate_market_potential(mean, sigma)\n",
    "        p, q = get_bass_params(conn)\n",
    "        _, cum_sales = bass_model(p, q, market_potential, num_periods)\n",
    "        results.loc[len(results)] = cum_sales\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Simulate 1000 iterations for 10 periods\n",
    "market_potential_mean = 100000\n",
    "market_potential_sigma = 20000\n",
    "num_periods = 10\n",
    "num_iterations = 1000\n",
    "\n",
    "simulation_results = simulate_bass(num_iterations, market_potential_mean, market_potential_sigma, num_periods)\n",
    "\n",
    "# Print average and standard deviation of cumulative sales for periods 1, 5, and 10\n",
    "for period in [1, 5, 10]:\n",
    "    print(f\"Period {period}:\")\n",
    "    print(f\"  Average Sales: {simulation_results[f'Period {period}'].mean():.0f}\")\n",
    "    print(f\"  Standard Deviation: {simulation_results[f'Period {period}'].std():.0f}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: Adstock Model: Accounting for Advertising Lag Effects* \n",
    "Subtitle: How do ads affect present and future sales?\n",
    "Content:\n",
    "- **Key Idea**: A fraction (λ) of previous ad stock is retained each period\n",
    "  - λ = 0.8: Ad from 1 period ago has 80% effect of current ad\n",
    "  - λ = 0.8: Ad from 2 periods ago has 0.8^2 = 51.2% effect of current ad\n",
    "- **Adstock Level**: Computed using equations\n",
    "  - *Quarter 1*: ADSTOCK = λ * INITIAL ADSTOCK + QUARTER 1 ADS \n",
    "  - *Quarter T*: ADSTOCK = λ * QUARTER (T-1) ADSTOCK + QUARTER T ADS\n",
    "- **Sales Forecast**: Predicted Sales = (CONST * (TREND^Quarter# + ADEFFECT * ADSTOCK) * (PRICE)^(-elasticity) * (Seasonal Index)\n",
    "\n",
    "\n",
    "### *Title: Interpreting Adstock Model Parameters*\n",
    "Subtitle: What insights can we glean from optimized Adstock models? \n",
    "Content:  \n",
    "- **Sales Trend**: % increase in sales per period\n",
    "  - E.g. 9.7% quarterly growth rate\n",
    "- **Ad Effectiveness Decay**: % of ad power lost each period   \n",
    "  - E.g. 17% lost quarterly if λ = 0.83\n",
    "- **Price Elasticity**: % demand change for 1% price change\n",
    "  - E.g. -1.49 elasticity means 1% price hike → 1.49% demand drop\n",
    "- **Seasonality Indices**: % deviation from average period sales  \n",
    "  - E.g. Q4 sales 67% above avg, Q2 30% below (indices 1.67 vs 0.70)\n",
    "\n",
    "\n",
    "### *Title: Implementing the Adstock Model in Excel*\n",
    "Content:\n",
    "1. In cell F6, use Equation 1 to compute quarter 1's Adstock: \n",
    "   - Formula: =E6+initialadstock*lambda\n",
    "2. In cells F7:F29, use Equation 2 to compute remaining quarters' Adstock:\n",
    "   - Formula: =E7+lambda*F6 (copy down)  \n",
    "3. In cells H6:H29, use Equation 3 to compute sales forecasts:\n",
    "   - Formula: =(const*(trend)^D6+adeffect*F6)*VLOOKUP(C6,season,2)*(G6)^(-elasticity)\n",
    "4. In cells J6:J29, compute each quarter's absolute percentage error:  \n",
    "   - Formula: =ABS(I6-H6)/I6\n",
    "5. In cell I4, compute MAPE:\n",
    "   - Formula: =AVERAGE(J6:J29)\n",
    "6. Use Solver (per Figure 32-1) to minimize MAPE by changing model parameters:\n",
    "   - Objective: Set MAPE (cell I54) to Min\n",
    "   - Variable Cells: const, trend, lambda, adeffect, elasticity, initialadstock  \n",
    "   - Constraints: parameter bounds, seasonality sum to 1\n",
    "   - Select GRG Nonlinear solving method \n",
    "7. Interpret optimized parameters (per Figure 34-2)\n",
    "\n",
    "\n",
    "\n",
    "### *The Python code follows these steps:*\n",
    "1. Load the Excel data into a pandas DataFrame\n",
    "2. Create a SQLite database and connection  \n",
    "3. Write the DataFrame to a table in SQLite\n",
    "4. Query the full data set from SQLite into a new DataFrame\n",
    "5. Perform Adstock calculations using pandas shift() and fillna()\n",
    "6. Create seasonal dummy variables via get_dummies()\n",
    "7. Set up expanded Adstock sales forecast equation \n",
    "8. Define MAPE objective function \n",
    "9. Use Scipy's minimize() to find optimal parameters\n",
    "   - Provide initial guesses, parameter bounds, seasonality constraint\n",
    "   - Use SLSQP solver for constrained optimization\n",
    "10. Print the minimized MAPE and optimal parameter values\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# Load data from Excel into pandas DataFrame  \n",
    "data = pd.read_excel('Adstock.xlsx')\n",
    "\n",
    "# Create SQLite database and connection\n",
    "conn = sqlite3.connect('adstock.db') \n",
    "data.to_sql('adstock', conn, index=False)\n",
    "\n",
    "# Query data from SQLite\n",
    "sql = '''\n",
    "  SELECT *\n",
    "  FROM adstock\n",
    "'''\n",
    "df = pd.read_sql(sql, conn)\n",
    "\n",
    "# Adstock calculations\n",
    "df['adstock'] = (df['ads'].shift(1).fillna(0) + \n",
    "                initialadstock) * lambda\n",
    "df_in = df[['qtr','ads','price','actual']]  \n",
    "\n",
    "# Seasonal dummies\n",
    "df_in = pd.concat([df_in, pd.get_dummies(df.qtr)], axis=1) \n",
    "\n",
    "# Expended sales forecast equation  \n",
    "sales_forecast = const * (trend**np.arange(len(df))) + adeffect*df.adstock\n",
    "sales_forecast *= (df.price ** -elasticity) * (df.loc[:,1:4] * season).sum(1) \n",
    "  \n",
    "# MAPE objective function  \n",
    "def adstock_mape(x):\n",
    "    prediction = forecast_func(x, df)\n",
    "    return np.mean(np.abs((df.actual - prediction) / df.actual)) * 100\n",
    "\n",
    "# Use Scipy optimize to minimize MAPE\n",
    "from scipy.optimize import minimize\n",
    "x0 = [1,1.04,.7,1,500,35000,1.5,.7,.7,1.5]  # Initial parameter guesses\n",
    "bnds = ((0, None),(1,1.5),(0,1),(0,None),(0,1e5),(0,1e6),(0,10),\n",
    "        (0,1),(0,1),(1,None)) \n",
    "cons = [{'type':'eq', 'fun': lambda x: np.sum(x[6:10])-4}]  # Season sum to 1\n",
    "res = minimize(adstock_mape, x0, method='SLSQP', bounds=bnds, constraints=cons)\n",
    "print(f\"Optimal Parameters: {res.x}\")\n",
    "print(f\"Minimized MAPE: {res.fun:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: Lilien-Kotler-Moorthy Ad Effectiveness Model*\n",
    "Subtitle: How do advertising levels, changes, and carryover impact sales?\n",
    "Content:\n",
    "- **Sales Response Function**: Qt = a + λQt-1 + bLN(At) + cmax(0,ΔAt)\n",
    "  - Qt: Period t sales \n",
    "  - At: Period t advertising\n",
    "  - ΔAt: % change in ads vs prior period\n",
    "- **Key Model Features**:\n",
    "  - *Diminishing returns* to advertising spend via LN(At)\n",
    "  - *Advertising carryover* effect via lagged sales term λQt-1\n",
    "  - *Pulse effects* from ad changes via cmax(0,ΔAt)\n",
    "\n",
    "\n",
    "### *Title: Comparing Ad Response Models*\n",
    "Subtitle: What are the key differences between Adstock and LKM? \n",
    "Content:\n",
    "- **Adstock Model**: \n",
    "  - *Geometric decay* of ad effect over time\n",
    "  - Ad stock *level* drives sales  \n",
    "  - Excel-friendly *multiplicative* model form\n",
    "- **LKM Model**:\n",
    "  - *Lagged sales term* captures ad carryover\n",
    "  - *Log* and *max* terms add flexibility\n",
    "  - *Additive* (not multiplicative) model structure\n",
    "- **Commonalities**: \n",
    "  - Quantify ad *carryover* and *diminishing returns*\n",
    "  - Require *unconstrained optimization* to fit\n",
    "\n",
    "\n",
    "### *Title: Fitting the LKM Model in Excel*\n",
    "Content:\n",
    "1. In cells F8:F42, compute ΔAt for each month: \n",
    "   - Formula: =(D8-D7)/D7\n",
    "2. In cells G8:G42, compute sales forecasts using LKM equation:\n",
    "   - Formula: =a+lambda*E8+b*LN(D8)+c_*MAX(0,F8) \n",
    "3. In cells H8:H42, compute squared errors:\n",
    "   - Formula: =(G8-C8)^2\n",
    "4. In cell H5, compute SSE:   \n",
    "   - Formula: =SUM(H8:H42)\n",
    "5. In cells I8:I42, compute forecast errors:\n",
    "   - Formula: =C8-G8  \n",
    "6. Use Solver (per Fig 34-4) to minimize SSE by changing a, lambda, b, c_:\n",
    "   - Objective: Set cell H5 to Min  \n",
    "   - Variable Cells: a, lambda, b, c_ \n",
    "   - Constraints: a>=0, 0<=lambda<=1, b>=0, c_>=-10 \n",
    "   - Select GRG Nonlinear engine\n",
    "7. Interpret fitted model (per steps in passage)\n",
    "\n",
    "\n",
    "### *The Python code follows these steps:*\n",
    "1. Load data from Excel, skipping headers and selecting relevant columns\n",
    "2. Create SQLite database and write data to a table\n",
    "3. Query full data set from SQLite into a DataFrame \n",
    "4. Create lagged variables sales (t-1) and ΔAt\n",
    "5. Define RSS objective function based on LKM equation\n",
    "6. Provide initial parameter guesses and bounds\n",
    "7. Use Scipy's minimize() to find optimal parameters  \n",
    "   - Nelder-Mead solver for unconstrained problem\n",
    "   - Print optimal values of a, b, c, λ\n",
    "8. Could add more code to analyze fit, plot actuals vs predicted, etc.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load data from Excel into pandas DataFrame\n",
    "data = pd.read_excel('addata.xls', skiprows=5, nrows=37, \n",
    "                     usecols='B:D', names=['month','sales','adv'])\n",
    "\n",
    "# Create SQLite database and connection \n",
    "conn = sqlite3.connect('addata.db')\n",
    "data.to_sql('addata', conn, index=False)\n",
    "\n",
    "# Query data from SQLite  \n",
    "query = '''\n",
    "  SELECT * \n",
    "  FROM addata\n",
    "'''\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# Lag variables\n",
    "df['lagsales'] = df.sales.shift(1)  \n",
    "df['deltaa'] = df.adv.pct_change()\n",
    "\n",
    "# Residual sum of squares (RSS) objective function\n",
    "def rss_lkm(params):\n",
    "    a, b, c, lam = params\n",
    "    sales_hat = (a + lam*df.lagsales + b*np.log(df.adv) + \n",
    "                 c*np.maximum(0,df.deltaa)) \n",
    "    return np.sum((df.sales[1:] - sales_hat[1:])**2)\n",
    "\n",
    "# Set initial param guesses and bounds\n",
    "guesses = [1, 1, 0, 0.5] \n",
    "bounds = [(0,None), (0,None), (None,None), (0,1)]\n",
    "\n",
    "# Use minimize() to estimate optimal params \n",
    "results = minimize(rss_lkm, guesses, method='L-BFGS-B', bounds=bounds)\n",
    "print(f\"Optimal a = {results.x[0]:.3f}\")  \n",
    "print(f\"Optimal b = {results.x[1]:.3f}\")\n",
    "print(f\"Optimal c = {results.x[2]:.3f}\") \n",
    "print(f\"Optimal λ = {results.x[3]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: Optimizing Advertising Spend*\n",
    "Subtitle: Continuous vs. Pulsing Strategies\n",
    "Content:  \n",
    "- How can we maximize profit through advertising?\n",
    "  - **Continuous spending:** constant monthly investment\n",
    "  - **Pulsing:** alternating high/low ad levels\n",
    "- Key factors:\n",
    "  - *Planning horizon length*\n",
    "  - *Problem parameters*\n",
    "- In practice: \n",
    "  - Continuous spending often optimal\n",
    "\n",
    "\n",
    "### *Title: Factors Influencing Optimal Ad Strategy*\n",
    "Subtitle: Planning Horizon and Problem Parameters\n",
    "Content:\n",
    "- What determines the best approach?\n",
    "  - **Planning horizon length**\n",
    "    - *Longer → favors continuous*\n",
    "    - *Shorter → may favor pulsing*  \n",
    "  - **Problem parameters** \n",
    "    - *Market factors, objectives, constraints*\n",
    "- Analyzing unique situational variables is key\n",
    "\n",
    "\n",
    "\n",
    "### *Lab Slide 1 - Excel Steps:*\n",
    "1. Input assumptions in cells G1:G5 \n",
    "   - 30% profit margin \n",
    "   - $1,000 unit price\n",
    "   - $100 per ad cost\n",
    "   - Month 1 sales: 12 units\n",
    "   - Month 1 ads: 9\n",
    "2. In F9, calculate profit: \n",
    "   =price*C9*profit_margin-100*D9\n",
    "   Copy to F10:F43 for months 2-36\n",
    "3. In C9, forecast sales:\n",
    "   =a+lambda*C8+b*LN(D9)+c_*MAX(0,E9)  \n",
    "   Copy to C10:C43\n",
    "4. Use Solver to maximize total profit (F44) \n",
    "   By changing ad levels in D10:D43\n",
    "   Constrain each month's ads >= 0.10\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load data from Excel \n",
    "data = pd.read_excel('addata.xlsx', sheet_name='optimize', usecols='G1:G5', nrows=5)\n",
    "\n",
    "# Create database connection\n",
    "conn = sqlite3.connect('addata.db')\n",
    "\n",
    "# Write data to SQLite table\n",
    "data.to_sql('assumptions', conn, index=False)\n",
    "\n",
    "# Query data from SQLite\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "  SELECT *\n",
    "  FROM assumptions\n",
    "''')\n",
    "\n",
    "assumptions = cur.fetchall()\n",
    "print(assumptions)\n",
    "\n",
    "# Placeholder for optimization code\n",
    "def optimize_ads(assumptions):\n",
    "  # Implement optimization logic\n",
    "  \n",
    "  # Return optimized ad levels\n",
    "  return optimal_ads\n",
    "\n",
    "# Get assumptions from SQLite\n",
    "profit_margin, price, ad_cost, month1_sales, month1_ads = assumptions[0]\n",
    "\n",
    "# Run optimization\n",
    "optimal_ads = optimize_ads(assumptions)\n",
    "\n",
    "print(f\"Optimal ad levels: {optimal_ads}\")\n",
    "\n",
    "# Close database connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
