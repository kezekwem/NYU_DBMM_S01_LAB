{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: Introduction to Decision Trees for Marketing Segmentation*\n",
    "Subtitle: Identifying Target Customers Using Demographic Data\n",
    "Content:\n",
    "- Decision trees enable marketers to predict customer behavior based on demographic attributes\n",
    "- Nodes are split based on metrics like entropy to create pure child nodes\n",
    "  - *Pure nodes*: all data points have same value for dependent variable\n",
    "  - *Impure nodes*: data points have different values, require further splitting\n",
    "- Goal is to create a classification rule determining likelihood of a customer action (e.g. purchasing a product)\n",
    "\n",
    "\n",
    "### *Title: Constructing a Decision Tree*\n",
    "Subtitle: A Step-by-Step Process Using Entropy and Impurity Metrics\n",
    "Content: \n",
    "- Begin with a root node containing all data points\n",
    "- For each independent variable, calculate entropy of potential child nodes \n",
    "  - *Entropy*: measures node impurity, 0 for pure node, 1 for max impurity\n",
    "- Choose split that minimizes weighted average impurity across child nodes\n",
    "  - *Impurity*: Σ (Entropy(child node) * % of observations in child node)\n",
    "- Continue splitting impure nodes until all terminal nodes are pure\n",
    "\n",
    "\n",
    "### *Lab Slide 1 - Excel Steps*\n",
    "Title: Calculating Impurity Metrics in Excel\n",
    "Content:\n",
    "1. Use COUNTIFS to tally number of people in each category who do/don't buy yogurt \n",
    "   - Example: =COUNTIFS($C$3:$C$12,$C15,$F$3:$F$12,D$14) \n",
    "2. Use SUM to total number of people for each attribute value\n",
    "   - Example: =SUM(D15:E15)\n",
    "3. Calculate fraction of observations for each attribute value \n",
    "   - Example: =F15/SUM($F$15:$F$16)\n",
    "4. Compute entropy components: P(i|X=a)*Log2(P(i|X=a)\n",
    "   - Example: =IFERROR((D15/$F15)*LOG(D15/$F15,2),0)\n",
    "5. Sum entropy components to get total entropy for each node split\n",
    "   - Example: =SUM(H15:I15) \n",
    "6. Calculate split impurity as weighted avg of child node entropies\n",
    "   - Example: =-SUMPRODUCT(G15:G16,J15:J16)\n",
    "7. Choose split with lowest impurity and continue until all terminal nodes are pure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sqlite3\n",
    "\n",
    "# Read Excel data into pandas DataFrame\n",
    "data = pd.read_excel('Greekyogurt.xlsx', sheet_name='Sheet1', \n",
    "                     usecols='A:F', skiprows=2, nrows=10)\n",
    "\n",
    "# Create SQLite3 database and write DataFrame to table\n",
    "conn = sqlite3.connect('greekyogurt.db')\n",
    "data.to_sql('customers', conn, index=False)\n",
    "\n",
    "# Compute entropy for gender split\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "    SELECT \n",
    "        Gender,\n",
    "        SUM(CASE WHEN Buys = 'Yes' THEN 1 ELSE 0 END) AS Buys_Yes,\n",
    "        SUM(CASE WHEN Buys = 'No' THEN 1 ELSE 0 END) AS Buys_No,\n",
    "        COUNT(*) AS Total\n",
    "    FROM customers\n",
    "    GROUP BY Gender\n",
    "''')\n",
    "\n",
    "gender_counts = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n",
    "gender_entropy = (-gender_counts['Buys_Yes']/gender_counts['Total'] * \n",
    "                 np.log2(gender_counts['Buys_Yes']/gender_counts['Total'])).sum()\n",
    "\n",
    "print(f\"Entropy for gender split: {gender_entropy:.3f}\")\n",
    "\n",
    "# Similar process for income and marital status splits\n",
    "# ...\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: Understanding S Curves in Marketing Analytics*\n",
    "Subtitle: Analyzing product adoption and sales over time\n",
    "Content:\n",
    "• What are S curves and why are they important in marketing?\n",
    "   - S curves show cumulative sales or adoption over time\n",
    "   - Examples: VAX minicomputers, cars in Italy, railroads in US\n",
    "• Key insights from S curves:\n",
    "   - **Upper limit of sales** - maximum potential \n",
    "   - **Inflection point** - when sales growth starts slowing\n",
    "• Strategically, products before inflection point have more growth potential\n",
    "\n",
    "\n",
    "### *Title: The Mathematics Behind S Curves*\n",
    "Subtitle: Normal distributions and percentile calculations\n",
    "Content: \n",
    "• Why do S curves occur? Individual adoption times follow a normal distribution\n",
    "   - Mean time to adopt: 5 years\n",
    "   - Standard deviation: 1.25 years\n",
    "• Recreating the S curve:\n",
    "   - Calculate percentiles of adoption times \n",
    "      - e.g. 10th percentile = average time 10th person adopts\n",
    "   - Count cumulative adoptions at each time point\n",
    "• Inflection point occurs at mean adoption time (t=5)\n",
    "\n",
    "\n",
    "### *Title: Recreating the S Curve in Excel*\n",
    "Content:\n",
    "1. In H9:H107, compute average adoption time for each percentile\n",
    "   - Use NORMINV function: `=NORMINV(G9/100,$I$4,$I$5)`\n",
    "   - G9 is percentile, $I$4 is mean, $I$5 is standard deviation \n",
    "2. In K11:K70, count cumulative adoptions at each time point\n",
    "   - Use COUNTIF: `=COUNTIF($H$9:$H$107,\"<=\"&J11)`\n",
    "   - Counts adoptions up to time in J11\n",
    "3. Graph J10:K70 as scatter chart to produce S curve \n",
    "   - Inflection point around t=5 (mean adoption time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load Excel data into pandas\n",
    "data = pd.read_excel('Scurvenormal.xlsx', sheet_name='Sheet1', usecols='G:I', skiprows=3, nrows=100)\n",
    "\n",
    "# Calculate adoption times using normal distribution\n",
    "mean = 5\n",
    "std = 1.25\n",
    "data['Time'] = norm.ppf(data['Person']/100, loc=mean, scale=std)\n",
    "\n",
    "# Create SQLite database and write data\n",
    "conn = sqlite3.connect('adoption.db')\n",
    "data.to_sql('adoptions', conn, index=False)\n",
    "\n",
    "# Query cumulative adoptions by time\n",
    "query = '''\n",
    "SELECT Time, COUNT(*) as People\n",
    "FROM adoptions \n",
    "WHERE Time <= ?\n",
    "GROUP BY Time\n",
    "'''\n",
    "\n",
    "# Initialize plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Loop through times and plot cumulative adoptions\n",
    "for t in range(11):\n",
    "    result = conn.execute(query, (t,)).fetchone()\n",
    "    if result:\n",
    "        ax.scatter(result[0], result[1])\n",
    "\n",
    "# Customize and display plot        \n",
    "ax.set_xlim([0,10])        \n",
    "ax.set_ylim([0,100])\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Cumulative Sales')\n",
    "ax.set_title('S Curve of Product Adoption')\n",
    "plt.show()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: Modeling Product Diffusion with the Logistic Curve*\n",
    "Subtitle: Understanding the path of product adoption over time\n",
    "Content:\n",
    "• What is the Logistic (Pearl) curve and why is it useful in marketing?\n",
    "   - Models cumulative sales, market penetration, or sales per capita \n",
    "   - Defined by equation: x(t) = L / (1 + ae^-bt)\n",
    "      - *L*: upper limit, *a* and *b*: determine slope\n",
    "• Key insights from Logistic curve:\n",
    "   - **Upper limit (L)** - maximum potential sales or adoption\n",
    "   - **Inflection point** - when growth rate starts slowing \n",
    "      - Occurs when t = Ln(a) / b\n",
    "\n",
    "\n",
    "### *Title: Estimating Logistic Curve Parameters in Excel*\n",
    "Subtitle: Using Solver to fit real-world data\n",
    "Content:\n",
    "• How can we estimate the L, a, and b parameters of a Logistic curve?\n",
    "   - Input historical data and set up Logistic equation in Excel\n",
    "   - Use Solver to minimize squared error by changing L, a, b\n",
    "      - *Squared error*: (Actual - Estimated)^2\n",
    "• Example: Modeling global cell phone adoption per 100 people\n",
    "   - Estimated equation: x(t) = 118.17 / (1 + 11.618e^-0.319t) \n",
    "   - Inflection point occurred in 2008 (t=7.67 years after 2001)\n",
    "   - Forecast 2012-2014 adoption by extending model\n",
    "\n",
    "\n",
    "### *Title: Fitting a Logistic Curve in Excel*\n",
    "Content:\n",
    "1. Enter historical data (year in column C, actuals in column E)\n",
    "2. In F2:H2, input initial guesses for L, a, and b parameters\n",
    "3. In F5:F15, estimate cells using Logistic formula\n",
    "   - Formula in F5: `=L/(1+a*EXP(-b*C5))`\n",
    "4. In G5:G15, calculate squared error for each estimate \n",
    "   - Formula in G5: `=(E5-F5)^2`\n",
    "5. In C3, sum squared errors with `=SUM(G5:G15)`  \n",
    "6. Use Solver to minimize C3 by changing cells $F$2:$H$2\n",
    "   - Set constraints: $F$2:$H$2 >= 0, L <= 200\n",
    "7. Extend model by copying formula in F15 to F16:F18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from Excel into pandas DataFrame\n",
    "data = pd.read_excel('worldcellpearl.xlsx', sheet_name='Sheet1', \n",
    "                     usecols='C,E', skiprows=4, nrows=11)\n",
    "\n",
    "# Rename columns and reset index to year\n",
    "data.columns = ['Year', 'Actual']\n",
    "data.set_index('Year', inplace=True)\n",
    " \n",
    "# Create SQLite database and write data\n",
    "conn = sqlite3.connect('cellphones.db')\n",
    "data.to_sql('adoption', conn)\n",
    "\n",
    "# Define the logistic function\n",
    "def logistic(t, L, a, b):\n",
    "    return L / (1 + a * np.exp(-b*t))\n",
    "\n",
    "# Extract year and actuals from database\n",
    "query = 'SELECT * FROM adoption'\n",
    "t, y = zip(*conn.execute(query))\n",
    "\n",
    "# Fit logistic curve\n",
    "popt, pcov = curve_fit(logistic, t, y, p0=(100, 10, 0.1))\n",
    "L, a, b = popt\n",
    "\n",
    "print(f\"Estimated equation: {L:.2f} / (1 + {a:.2f} * exp(-{b:.2f} * t))\")\n",
    "\n",
    "# Calculate inflection point\n",
    "inflection_t = np.log(a) / b\n",
    "inflection_year = int(t[0] + inflection_t)\n",
    "print(f\"Inflection point occurred in {inflection_year}\")\n",
    "\n",
    "# Plot data and fitted curve\n",
    "t_ext = np.append(t, [t[-1]+1, t[-1]+2, t[-1]+3])  # Extend for forecast\n",
    "y_ext = logistic(t_ext, L, a, b)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t, y, 'o', label='Actual')  \n",
    "plt.plot(t_ext, y_ext, label='Logistic Model')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Cell Phones per 100 People')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Title: Incorporating Seasonality into S-Curve Fitting*\n",
    "Subtitle: Adapting logistic curves for quarterly or monthly data\n",
    "Content:\n",
    "- When fitting a logistic curve to quarterly/monthly data, seasonality must be incorporated\n",
    "  - Multiply S-curve forecast by appropriate seasonal index\n",
    "  - Add seasonal indices as changing cells\n",
    "  - Choose forecast parameters to minimize sum of squared errors\n",
    "- Example: Fitting seasonal logistic curve to 2002-2006 quarterly iPod sales\n",
    "  - Data in \"iPodsseasonal.xls\" file\n",
    "\n",
    "\n",
    "### *Title: Fitting a Seasonal S-Curve in Excel*\n",
    "Subtitle: Step-by-step process using iPod sales data\n",
    "Content: \n",
    "1. Compute sales per 100 people for each quarter\n",
    "2. Calculate forecast by multiplying S-curve value by seasonal index\n",
    "3. Compute squared error for each prediction  \n",
    "4. Calculate sum of squared errors\n",
    "5. Add constraint for seasonal indices to average 1\n",
    "6. Use Solver to find optimal parameters (e.g., a=1000)\n",
    "\n",
    "\n",
    "### *Title: Fitting a Seasonal S-Curve in Excel*\n",
    "Content:\n",
    "1. In cells F5:F19, compute sales per 100 people for each quarter using the formula: \n",
    "   =100*D5/E5\n",
    "2. In cells G5:G19, calculate the forecast for each quarter using the formula:\n",
    "   =(L/(1+a*EXP(-b*A5)))*HLOOKUP(C5,seaslook,2)\n",
    "3. In cells H5:H19, compute the squared error for each prediction with the formula:\n",
    "   =(F5-G5)^2  \n",
    "4. In cell H3, calculate the sum of squared errors using:\n",
    "   =SUM(H5:H19)\n",
    "5. Add the constraint $N$2=1 to ensure seasonal indices average to 1\n",
    "6. Use Solver with the GRG MultiStart Engine:\n",
    "   - Set objective to H3 (sum of squared errors)\n",
    "   - By changing variable cells $G$2:$M$2  \n",
    "   - Subject to constraint $N$2=1\n",
    "   - Raise upper bound of 'a' to 10,000 if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data from Excel into a pandas DataFrame\n",
    "data = pd.read_excel('iPodsseasonal.xlsx', sheet_name='Sheet1', usecols='A:F', skiprows=3, nrows=16)\n",
    "\n",
    "# Connect to SQLite3 database\n",
    "conn = sqlite3.connect('iPodsseasonal.db')\n",
    "\n",
    "# Write DataFrame to SQLite3 table\n",
    "data.to_sql('sales_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Query data from SQLite3\n",
    "query = '''\n",
    "    SELECT * \n",
    "    FROM sales_data\n",
    "'''\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Compute sales per 100 people\n",
    "df['Sales_per_100'] = 100 * df['Sales'] / df['Pop']\n",
    "\n",
    "# Define the logistic function\n",
    "def logistic(x, L, a, b):\n",
    "    return L / (1 + a * np.exp(-b * x))\n",
    "\n",
    "# Define seasonal indices\n",
    "seasonal_indices = [0.9, 0.83, 0.77, 1.49]\n",
    "\n",
    "# Optimize parameters using least squares\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "quarters = df.index.tolist()\n",
    "sales_per_100 = df['Sales_per_100'].tolist()\n",
    "\n",
    "def logistic_seasonal(x, L, a, b):\n",
    "    return logistic(x, L, a, b) * seasonal_indices[int(x % 4)]\n",
    "\n",
    "params, _ = curve_fit(logistic_seasonal, quarters, sales_per_100, p0=[3, 1000, 0.5])\n",
    "\n",
    "L, a, b = params\n",
    "\n",
    "# Generate predictions\n",
    "predictions = [logistic_seasonal(x, L, a, b) for x in quarters]\n",
    "\n",
    "# Calculate sum of squared errors\n",
    "sse = mean_squared_error(sales_per_100, predictions) * len(sales_per_100)\n",
    "print(f\"Sum of Squared Errors: {sse:.2f}\")\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(quarters, sales_per_100, 'bo-', label='Actual')\n",
    "plt.plot(quarters, predictions, 'r*-', label='Predicted') \n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Sales per 100 People')\n",
    "plt.title('iPod Sales: Actual vs Predicted')\n",
    "plt.xticks(quarters, [f\"Q{(q%4)+1} {2002+(q//4)}\" for q in quarters], rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
